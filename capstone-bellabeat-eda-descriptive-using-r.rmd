# %% [code]
---
title: "Google Capstone Project - EDA & Basic Descriptive Analysis using R"
author: "**Wisjnu Judho**"
date: '`r format(Sys.time())`'
output: 
    html_document:
        toc: true
        theme: readable
---

## **ASK** {.tabset}

### **Objective** {-}

**Urška Sršen would like to know any recommendation for how the data can inform Bellabeat Marketing strategy.**

1. What are some trends in smart device usage?
2. How could these trends apply to Bellabeat customers?
3. How could these trends help influence Bellabeat marketing strategy?

### **About Bellabeat** {-}

Bellabeat device works like smartwatch and yet this is not a watch rather a wearable device that tracks sleeps, steps distance, and daily activity. It claimed using a sensor in the device, to transfer the data through an app in your Android or Apple smartphones.

## **PREPARE** {.tabset}

### Tools {-}

**We decide to work everything using R Studio in format `Rmd`. Heres what we think:**

1. dataset already available (since We dont have to get it from server database).
2. If needed, R has the ability to connect to database and function to call schema using SQL queries directly in R.
3. We refrain using Excel (Power Query) as the Dates timezone in `csv` usually conflicts with Windows OS regional.
4. Excel refuse to open large file `this data set is too large for the excel grid`.
5. We dont use Tableau because R simply has the library to create chart that sufficient for the descriptive analytic purpose.
6. `Rmd` allows me to create `html`, `doc`, or `pdf` file for documentation purpose.

### Datasets {-}

Urška Sršen has provided Us with datasets in [[kaggle](https://www.kaggle.com/datasets/arashnic/fitbit)]. 
<br>
**After downloaded, Our files list in folder should be 18 csv. Some files are zipped, so make sure to unzip it.**

```{r, collapse=TRUE}
list.files("../input/fitbit/Fitabase Data 4.12.16-5.12.16")
```

**After exploring for some time, We decide to discard most of datasets, heres the reason:** 

* Because it contain duplicate column which already presented fully on `dailyActivity_merged.csv`,
* We couldnt find the formula to calculate column `TotalDistance` on `dailyActivity_merged.csv`, 
* We dont use hourly, minutes, and second dataset, We believe daily data is sufficient enough to represent them.
* We save datasets into a variable each, so that we dont have to `read.csv("filename.csv")` everytime we call functions. 
* We also use a variable names that concise and easy to understand for other analyst to read.

```{r, collapse=TRUE}
# importing datasets
d_activity <- read.csv("../input/fitbit/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv")
d_calories <- read.csv("../input/fitbit/Fitabase Data 4.12.16-5.12.16/dailyCalories_merged.csv")
d_intensities <- read.csv("../input/fitbit/Fitabase Data 4.12.16-5.12.16/dailyIntensities_merged.csv")
d_steps <- read.csv("../input/fitbit/Fitabase Data 4.12.16-5.12.16/dailySteps_merged.csv")
d_sleep <- read.csv("../input/fitbit/Fitabase Data 4.12.16-5.12.16/sleepDay_merged.csv")
weightlog <- read.csv("../input/fitbit/Fitabase Data 4.12.16-5.12.16/weightLogInfo_merged.csv")
```

### Loading Package {-}

* **tidyverse:** EDA purpose 
* **lubridate:** Working with Date

```{r, collapse=TRUE}
# loading package
library(tidyverse)
library(lubridate)
```
<br>

**Loading tidyverse alone will automatically load a bunchs of different library altogether.**

```{r, collapse=TRUE}
# list tidyverse library
tidyverse_packages()
```

### Understanding Datasets {-}

The process of tidying data could lead to unexpected things, most of it something silly like columns or rows name being replaced or deleted unintended. We can use both total column and rows as a benchmark. **Use this section from time to time to cross-check whether our datasets have been changed or not.** 

```{r, collapse=TRUE}
# show total rows|columns each dataset (not distinct)
dim_desc(d_activity)
dim_desc(d_calories)
dim_desc(d_intensities)
dim_desc(d_steps)
dim_desc(d_sleep)
dim_desc(weightlog)
```
<br>

We pick column ID in every dataset and use `unique` function to distinct the customer.  Here we can see total participant in our subject analysis. **The majority dataset is telling us 33 people, but some are mention even less**. It is indicated that some customers log arent even recorded. it means some customer didnt use particular feature.

```{r, collapse=TRUE}
# Monitor Total Customer on every record
length(unique(d_activity$Id))
length(unique(d_calories$Id))
length(unique(d_intensities$Id))
length(unique(d_steps$Id))
length(unique(d_sleep$Id))
length(unique(weightlog$Id))
```
<br>

**Few thing to consider are:**

1. Date should be Date format (not character), this is why Excel has problem handling it.
2. ID should be a character, We dont want to accidentally calculate an ID, as its not a number rather an identity, so it not supposed to changed.

```{r, collapse=TRUE}
# General Overview, especially `date`, and `data type`
glimpse(d_activity)
glimpse(d_calories)
glimpse(d_intensities)
glimpse(d_steps)
glimpse(d_sleep)
glimpse(weightlog)
```


### Check for Missing Value {-}

**If the values are missing at random, the data will still representing the population, otherwise the analysis result will be biased.**  

```{r, collapse=TRUE}
# Check total NA in every column
colSums(is.na(d_activity))
colSums(is.na(d_calories))
colSums(is.na(d_intensities))
colSums(is.na(d_steps))
colSums(is.na(d_sleep))
colSums(is.na(weightlog))
```
<br>

**Lets make it readable without reveal the column**

```{r, collapse=TRUE}
# sum total NA on each dataset without reveal the column
sum(is.na(d_activity))
sum(is.na(d_calories))
sum(is.na(d_intensities))
sum(is.na(d_steps))
sum(is.na(d_sleep))
sum(is.na(weightlog))
```
**we got 65 NA in `weightlog`. Is 65 big or small?**
<br>

**Lets calculate total NA by total row, that way we could estimate portion NA in its particular rows**

```{r, collapse=TRUE}
# How much NA value in percentage
colSums(is.na(weightlog)) / nrow(weightlog)
```
**97% from just one column, staggering!. How could theres no value in there? as if every customer deliberately leave it empty. Up to this point, there is no point to remove the missing value**
<br>

## **PROCESS** {.tabset}


### Change Data Type {-}


```{r}
# Change date type from char to POSIXct and DATE
weightlog$logedit <- mdy_hms(weightlog$Date)
d_activity$ActivityDateNew <- mdy(d_activity$ActivityDate)
d_sleep$SleepDayNew <- mdy_hms(d_sleep$SleepDay)
```

```{r}
# Change ID from numeric to char
d_activity$Id <- as.character(d_activity$Id)
```


## **ANALYZE & SHARE** {.tabset}

### Mean Median Mode {-}

```{r, collapse=TRUE}
# descriptive analysis
summary(d_activity)
summary(d_calories)
summary(d_intensities)
summary(d_steps)
summary(d_sleep)
summary(weightlog)
```

### Plot Weightlog {-}

We can se there is a correlation between BMI and WeightKg, eventhough subtle

```{r}
# Create scatterplot weightlog (ggplot exclude missing value by default)
ggplot(data = weightlog, aes(x=BMI, y=WeightKg)) + 
  geom_point()
```


### Plot d_sleep {-}

We can see positive correlation between TotalMinutesAsleep and TotalTimeInBed, obviously and it is very high. 

```{r}
# Create scatterplot d_sleep
ggplot(data = d_sleep, aes(x=TotalMinutesAsleep, y=TotalTimeInBed)) +
  geom_point()
```

### Plot d_activity {-}

It seems the spread are gathering from the `0` towards `1000`. It means some customer were exciting `in` doing workout just `for` starter then consecutively stop

```{r}
# Create scatterplot steps-distance
ggplot(data = d_activity, aes(x=TotalSteps, y=TotalDistance)) +
  geom_point()
```
calory burns are not high, they are scattered towards middle. Meaning it is correlated each other but obviously people dont like burn the idea of burning calory

```{r}
# Create scatterplot 
ggplot(data = d_activity, aes(x=Calories, y=TotalDistance)) +
  geom_point()
```





